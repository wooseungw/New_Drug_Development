{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoow070840\u001b[0m (\u001b[33mwaooang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Workspace\\New_drug_development\\wandb\\run-20230920_120826-1bvfxnq6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/New_drug/runs/1bvfxnq6' target=\"_blank\">tough-terrain-1</a></strong> to <a href='https://wandb.ai/waooang/New_drug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/New_drug' target=\"_blank\">https://wandb.ai/waooang/New_drug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/New_drug/runs/1bvfxnq6' target=\"_blank\">https://wandb.ai/waooang/New_drug/runs/1bvfxnq6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 174, got 175",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m\n\u001b[0;32m     62\u001b[0m initial_model\u001b[39m.\u001b[39mfit(X_initial, Y_initial)\n\u001b[0;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_queries)):\n\u001b[0;32m     65\u001b[0m     \u001b[39m# 예측 오차를 계산하여 불확실성을 측정\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     Y_pred \u001b[39m=\u001b[39m initial_model\u001b[39m.\u001b[39;49mpredict(unlabeled_data)  \u001b[39m# 레이블이 없는 데이터에 대한 예측\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m#uncertainty = np.abs(Y_pred - np.mean(Y_pred, axis=0))\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     uncertainty \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmax(Y_pred, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\sklearn.py:1114\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1113\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1114\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[0;32m   1115\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1116\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1117\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1118\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1119\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1120\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1121\u001b[0m         )\n\u001b[0;32m   1122\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1123\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\core.py:2269\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2265\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2266\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2267\u001b[0m         )\n\u001b[0;32m   2268\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features() \u001b[39m!=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m-> 2269\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2270\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature shape mismatch, expected: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2271\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2272\u001b[0m         )\n\u001b[0;32m   2274\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m   2275\u001b[0m     _array_interface,\n\u001b[0;32m   2276\u001b[0m     _is_cudf_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2279\u001b[0m     _transform_pandas_df,\n\u001b[0;32m   2280\u001b[0m )\n\u001b[0;32m   2282\u001b[0m enable_categorical \u001b[39m=\u001b[39m _has_categorical(\u001b[39mself\u001b[39m, data)\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 174, got 175"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import xgboost as xgb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"New_drug\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"architecture\": \"XGBoost\",\n",
    "    \"dataset\": \"Custom\",\n",
    "    \"epochs\": n_queries,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# CSV 파일 경로\n",
    "# labeled_data_csv = \"./origin_data/train.csv\"  # 레이블이 있는 데이터 파일\n",
    "# unlabeled_data_csv = \"./data/unlabeld_data2.csv\"  # 레이블이 없는 데이터 파일\n",
    "labeled_data_csv = \"./data/finger_maccs_fp_train.csv\"  # 레이블이 있는 데이터 파일\n",
    "unlabeled_data_csv = \"./data/unlabeled_maccs.csv\"  # 레이블이 없는 데이터 파일\n",
    "\n",
    "# CSV 파일 읽기\n",
    "labeled_data = pd.read_csv(labeled_data_csv).drop(columns=[\"id\", \"SMILES\"])\n",
    "unlabeled_data = pd.read_csv(unlabeled_data_csv).drop(columns=[\"SMILES\"])\n",
    "\n",
    "labeled_data = labeled_data.fillna(0)\n",
    "unlabeled_data = unlabeled_data.fillna(0)\n",
    "\n",
    "\n",
    "# 레이블이 있는 데이터 추출\n",
    "X_labeled = labeled_data.drop(columns=['MLM', 'HLM'])\n",
    "Y_labeled = labeled_data['MLM']\n",
    "\n",
    "###\n",
    "# # 초기 학습 데이터 선택 (랜덤으로 선택)\n",
    "# initial_idx = np.random.choice(range(len(X_labeled)), size=1000, replace=False)\n",
    "# X_initial = X_labeled.iloc[initial_idx]\n",
    "# Y_initial = Y_labeled.iloc[initial_idx]\n",
    "\n",
    "# 초기 학습 데이터 선택 (80%)\n",
    "test_size = 0.2  # 전체 데이터 중 테스트 데이터 비율 (예: 20%)\n",
    "X_initial, X_test, Y_initial, Y_test = train_test_split(X_labeled, Y_labeled, test_size=test_size, random_state=42)\n",
    "X_test1, X_test2, Y_test1, Y_test2 = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n",
    "###\n",
    "params={'reg_lambda': 3.970598535915323, 'gamma': 9.254009054984513, 'reg_alpha': 8.180688938901085, 'colsample_bytree': 0.7, 'subsample': 0.7, 'max_depth': 3, 'min_child_weight': 158}\n",
    "# 초기 모델 학습 (랜덤 포레스트 회귀 모델 사용)\n",
    "initial_model = xgb.XGBRegressor(**params,tree_method= \"hist\", gpu_id=0,seed=42)\n",
    "initial_model.fit(X_initial, Y_initial)\n",
    "\n",
    "for i in tqdm(range(n_queries)):\n",
    "    # 예측 오차를 계산하여 불확실성을 측정\n",
    "    Y_pred = initial_model.predict(unlabeled_data)  # 레이블이 없는 데이터에 대한 예측\n",
    "    #uncertainty = np.abs(Y_pred - np.mean(Y_pred, axis=0))\n",
    "    uncertainty = 1 - np.max(Y_pred, axis=1)\n",
    "\n",
    "    # 불확실성이 가장 큰 상위 배치 크기만큼 데이터 포인트 선택\n",
    "    #query_indices = np.argsort(uncertainty.sum(axis=1))[-batch_size:]\n",
    "    query_indices = np.argsort(uncertainty)[-batch_size:]\n",
    "\n",
    "    for query_idx in query_indices:\n",
    "        # 쿼리된 데이터는 학습 데이터에 추가\n",
    "        X_query = unlabeled_data.iloc[query_idx:query_idx+1]\n",
    "        Y_query = Y_pred[query_idx:query_idx+1]  # 모델 예측값을 사용\n",
    "\n",
    "        X_initial = pd.concat([X_initial, X_query])\n",
    "        Y_initial = np.vstack([Y_initial, Y_query])\n",
    "\n",
    "    # 쿼리된 데이터는 레이블이 없는 데이터에서 제거 (선택된 데이터만큼 제거)\n",
    "    unlabeled_data = unlabeled_data.drop(unlabeled_data.index[query_indices])\n",
    "\n",
    "    # 새로운 모델을 학습하여 계속 진행\n",
    "    initial_model = xgb.XGBRegressor(**params,tree_method= \"hist\", gpu_id=0,seed=42)\n",
    "\n",
    "    Y_pred = initial_model.predict(X_test1)\n",
    "\n",
    "    rmse_MLM = np.sqrt(mean_squared_error(Y_test1[\"MLM\"], Y_pred[:,0]))\n",
    "    rmse_HLM = np.sqrt(mean_squared_error(Y_test1[\"HLM\"], Y_pred[:,1]))\n",
    "    loss = (rmse_HLM+rmse_MLM)/2\n",
    "        # log metrics to wandb\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "\n",
    "Y_pred = initial_model.predict(X_test2)\n",
    "\n",
    "rmse_MLM = np.sqrt(mean_squared_error(Y_test2[\"MLM\"], Y_pred[:,0]))\n",
    "rmse_HLM = np.sqrt(mean_squared_error(Y_test2[\"HLM\"], Y_pred[:,1]))\n",
    "rmse = (rmse_HLM+rmse_MLM)/2\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Active Learning 반복 과정\n",
    "n_queries = 20\n",
    "batch_size = 1750  # 각 반복에서 선택할 배치 크기\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CSV 파일 경로\n",
    "# labeled_data_csv = \"./origin_data/train.csv\"  # 레이블이 있는 데이터 파일\n",
    "# unlabeled_data_csv = \"./data/unlabeld_data2.csv\"  # 레이블이 없는 데이터 파일\n",
    "labeled_data_csv = \"./data/finger_maccs_fp_train.csv\"  # 레이블이 있는 데이터 파일\n",
    "unlabeled_data_csv = \"./data/unlabeled_maccs.csv\"  # 레이블이 없는 데이터 파일\n",
    "\n",
    "# CSV 파일 읽기\n",
    "labeled_data = pd.read_csv(labeled_data_csv).drop(columns=[\"id\", \"SMILES\"])\n",
    "unlabeled_data = pd.read_csv(unlabeled_data_csv).drop(columns=[\"SMILES\"])\n",
    "\n",
    "labeled_data = labeled_data.fillna(0)\n",
    "unlabeled_data = unlabeled_data.fillna(0)\n",
    "\n",
    "\n",
    "# 레이블이 있는 데이터 추출\n",
    "X_labeled = labeled_data.drop(columns=['MLM', 'HLM'])\n",
    "Y_labeled = labeled_data[['MLM']]\n",
    "\n",
    "###\n",
    "# # 초기 학습 데이터 선택 (랜덤으로 선택)\n",
    "# initial_idx = np.random.choice(range(len(X_labeled)), size=1000, replace=False)\n",
    "# X_initial = X_labeled.iloc[initial_idx]\n",
    "# Y_initial = Y_labeled.iloc[initial_idx]\n",
    "\n",
    "# 초기 학습 데이터 선택 (80%)\n",
    "test_size = 0.2  # 전체 데이터 중 테스트 데이터 비율 (예: 20%)\n",
    "X_initial, X_test, Y_initial, Y_test = train_test_split(X_labeled, Y_labeled, test_size=test_size, random_state=42)\n",
    "X_test1, X_test2, Y_test1, Y_test2 = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n",
    "###\n",
    "\n",
    "# 초기 모델 학습 (랜덤 포레스트 회귀 모델 사용)\n",
    "initial_model = xgb.XGBRegressor(tree_method= \"hist\", gpu_id=0)\n",
    "initial_model.fit(X_initial, Y_initial)\n",
    "\n",
    "for i in tqdm(range(n_queries)):\n",
    "    # 예측 오차를 계산하여 불확실성을 측정\n",
    "    Y_pred = initial_model.predict(unlabeled_data)  # 레이블이 없는 데이터에 대한 예측\n",
    "    #uncertainty = np.abs(Y_pred - np.mean(Y_pred, axis=0))\n",
    "    uncertainty = 1 - np.max(Y_pred, axis=1)\n",
    "\n",
    "    # 불확실성이 가장 큰 상위 배치 크기만큼 데이터 포인트 선택\n",
    "    #query_indices = np.argsort(uncertainty.sum(axis=1))[-batch_size:]\n",
    "    query_indices = np.argsort(uncertainty)[-batch_size:]\n",
    "\n",
    "    for query_idx in query_indices:\n",
    "        # 쿼리된 데이터는 학습 데이터에 추가\n",
    "        X_query = unlabeled_data.iloc[query_idx:query_idx+1]\n",
    "        Y_query = Y_pred[query_idx:query_idx+1]  # 모델 예측값을 사용\n",
    "\n",
    "        X_initial = pd.concat([X_initial, X_query])\n",
    "        Y_initial = np.vstack([Y_initial, Y_query])\n",
    "\n",
    "    # 쿼리된 데이터는 레이블이 없는 데이터에서 제거 (선택된 데이터만큼 제거)\n",
    "    unlabeled_data = unlabeled_data.drop(unlabeled_data.index[query_indices])\n",
    "\n",
    "    # 새로운 모델을 학습하여 계속 진행\n",
    "    initial_model = xgb.XGBRegressor(tree_method= \"hist\", gpu_id=0)\n",
    "    initial_model.fit(X_initial, Y_initial)\n",
    "\n",
    "    Y_pred = initial_model.predict(X_test1)\n",
    "\n",
    "    rmse_MLM = np.sqrt(mean_squared_error(Y_test1[\"MLM\"], Y_pred[:,0]))\n",
    "    rmse_HLM = np.sqrt(mean_squared_error(Y_test1[\"HLM\"], Y_pred[:,1]))\n",
    "    loss = (rmse_HLM+rmse_MLM)/2\n",
    "    \n",
    "\n",
    "\n",
    "Y_pred = initial_model.predict(X_test2)\n",
    "\n",
    "rmse_MLM = np.sqrt(mean_squared_error(Y_test2[\"MLM\"], Y_pred[:,0]))\n",
    "rmse_HLM = np.sqrt(mean_squared_error(Y_test2[\"HLM\"], Y_pred[:,1]))\n",
    "rmse = (rmse_HLM+rmse_MLM)/2\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./AutogluonModels/initial_model_maccs_SS.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델 저장\n",
    "joblib.dump(initial_model, './AutogluonModels/initial_model_maccs_SS.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# # 모델 로드\n",
    "# loaded_model = joblib.load('initial_model1.pkl')\n",
    "\n",
    "# # 로드한 모델로 예측 등을 수행할 수 있음\n",
    "# loaded_predictions = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_pred_df = pd.DataFrame(Y_pred)\n",
    "# Y_pred_df.to_csv(\"./data/SS_valid_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_csv = \"./origin_data/test.csv\"\n",
    "test_csv = \"./data/finger_maccs_fp_test.csv\"\n",
    "test_data = pd.read_csv(test_csv).drop(columns=[\"id\", \"SMILES\"])\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "Y_test_pred = initial_model.predict(test_data)\n",
    "# predict\n",
    "df_submission = pd.read_csv(\"./origin_data/sample_submission.csv\")\n",
    "df_submission[\"MLM\"] = Y_test_pred[:,0]\n",
    "df_submission[\"HLM\"] = Y_test_pred[:,1]\n",
    "df_submission.to_csv(\"./submission/submission_SSwandb_maccs_LBW.csv\", index = False, encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autog = pd.read_csv(\"./submission/autog.csv\")\n",
    "ss2 = pd.read_csv(\"./submission/submission_SS2_LBW.csv\")\n",
    "ss = pd.read_csv(\"./submission/submission_SS_LBW.csv\")\n",
    "ss35 = pd.read_csv(\"./submission/submission_SS35k_LBW.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
