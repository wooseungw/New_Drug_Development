{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW,Adam\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def count_parameters_per_layer(model):\n",
    "    param_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        param_counts[name] = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    return param_counts\n",
    "\n",
    "# RMSE 손실 함수 정의\n",
    "def rmse_loss(y_pred, y_true):\n",
    "    mse = torch.nn.MSELoss()(y_pred, y_true)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECFPDataset(Dataset):\n",
    "    def __init__(self,ecfp,hlm,mlm):\n",
    "        self.ecfp = ecfp.tolist()\n",
    "        self.hlm = hlm.tolist()\n",
    "        self.mlm = mlm.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        mlm = torch.tensor(self.mlm[index],dtype=torch.float)\n",
    "        hlm = torch.tensor(self.hlm[index],dtype=torch.float)\n",
    "        ecfp = torch.tensor(self.ecfp[index],dtype=int)\n",
    "        return ecfp, mlm, hlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBasedRegressor(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(AttentionBasedRegressor, self).__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(pretrained_model_name, output_attentions=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(self.encoder.config.hidden_size, self.encoder.config.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 최종 예측을 위한 Linear 레이어\n",
    "        self.regressor = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        attention_scores = outputs.attentions[-1]\n",
    "        attention_weights = attention_scores.mean(dim=1)\n",
    "        attention_weights_avg = attention_weights.mean(dim=-1)\n",
    "        weighted_avg = torch.sum(sequence_output * attention_weights_avg.unsqueeze(-1), dim=1)\n",
    "\n",
    "\n",
    "        x = self.fc1(weighted_avg)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mlm_prediction = self.regressor(x)\n",
    "        hlm_prediction = self.regressor(x)\n",
    "        return mlm_prediction, hlm_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        # Weight matrices\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        # Attention mechanisms\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        # Compute attention coefficients\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # Masked attention\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)  # 활성화 함수 ELU를 적용하여 결과 반환\n",
    "        else:\n",
    "            return h_prime  # 결과 반환\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Multi-head attention layers\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Example usage:\n",
    "# model = GAT(nfeat=features.shape[1], nhid=8, nclass=labels.max().item() + 1, dropout=0.6, alpha=0.2, nheads=8)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
