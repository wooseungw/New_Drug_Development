{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from deepchem.models.torch_models import layers\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from deepchem.models.losses import L2Loss\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class MAT(nn.Module):\n",
    "    \"\"\"An internal TorchModel class.\n",
    "\n",
    "    In this class, we define the various layers and establish a sequential model for the Molecular Attention Transformer.\n",
    "    We also define the forward call of this model in the forward function.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Lukasz Maziarka et al. \"Molecule Attention Transformer\" Graph Representation Learning workshop and Machine Learning and the Physical Sciences workshop at NeurIPS 2019. 2020. https://arxiv.org/abs/2002.08264\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import deepchem as dc\n",
    "    >>> import pandas as pd\n",
    "    >>> import numpy as np\n",
    "    >>> smiles = ['CC', 'CCC',  'CCCC', 'CCCCC', 'CCCCCCC']\n",
    "    >>> vals = [1.35, 6.72, 5.67, 1.23, 1.76]\n",
    "    >>> df = pd.DataFrame(list(zip(smiles, vals)), columns = ['smiles', 'y'])\n",
    "    >>> loader = dc.data.CSVLoader(tasks=['y'], feature_field='smiles', featurizer=dc.feat.MATFeaturizer())\n",
    "    >>> df.to_csv('test.csv')\n",
    "    >>> dataset = loader.create_dataset('test.csv')\n",
    "    >>> model = dc.models.torch_models.MAT()\n",
    "    >>> # To simulate input data, we will generate matrices for a single molecule.\n",
    "    >>> vals = dataset.X[0]\n",
    "    >>> node = vals.node_features\n",
    "    >>> adj = vals.adjacency_matrix\n",
    "    >>> dist = vals.distance_matrix\n",
    "    >>> # We will now utilize a helper function defined in MATModel to get our matrices ready, and convert them into a batch consisting of a single molecule.\n",
    "    >>> helper = dc.models.torch_models.MATModel()\n",
    "    >>> node_features = helper.pad_sequence(torch.tensor(node).unsqueeze(0).float())\n",
    "    >>> adjacency = helper.pad_sequence(torch.tensor(adj).unsqueeze(0).float())\n",
    "    >>> distance = helper.pad_sequence(torch.tensor(dist).unsqueeze(0).float())\n",
    "    >>> inputs = [node_features, adjacency, distance]\n",
    "    >>> inputs = [x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs]\n",
    "    >>> # Get the forward call of the model for this batch.\n",
    "    >>> output = model(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dist_kernel: str = 'softmax',\n",
    "                 n_encoders=8,\n",
    "                 lambda_attention: float = 0.33,\n",
    "                 lambda_distance: float = 0.33,\n",
    "                 h: int = 16,\n",
    "                 sa_hsize: int = 1024,\n",
    "                 sa_dropout_p: float = 0.0,\n",
    "                 output_bias: bool = True,\n",
    "                 d_input: int = 1024,\n",
    "                 d_hidden: int = 1024,\n",
    "                 d_output: int = 1024,\n",
    "                 activation: str = 'leakyrelu',\n",
    "                 n_layers: int = 1,\n",
    "                 ff_dropout_p: float = 0.0,\n",
    "                 encoder_hsize: int = 1024,\n",
    "                 encoder_dropout_p: float = 0.0,\n",
    "                 embed_input_hsize: int = 36,\n",
    "                 embed_dropout_p: float = 0.0,\n",
    "                 gen_aggregation_type: str = 'mean',\n",
    "                 gen_dropout_p: float = 0.0,\n",
    "                 gen_n_layers: int = 1,\n",
    "                 gen_attn_hidden: int = 128,\n",
    "                 gen_attn_out: int = 4,\n",
    "                 gen_d_output: int = 1,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialization for the internal MAT class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dist_kernel: str\n",
    "            Kernel activation to be used. Can be either 'softmax' for softmax or 'exp' for exponential, for the self-attention layer.\n",
    "        n_encoders: int\n",
    "            Number of encoder layers in the encoder block.\n",
    "        lambda_attention: float\n",
    "            Constant to be multiplied with the attention matrix in the self-attention layer.\n",
    "        lambda_distance: float\n",
    "            Constant to be multiplied with the distance matrix in the self-attention layer.\n",
    "        h: int\n",
    "            Number of attention heads for the self-attention layer.\n",
    "        sa_hsize: int\n",
    "            Size of dense layer in the self-attention layer.\n",
    "        sa_dropout_p: float\n",
    "            Dropout probability for the self-attention layer.\n",
    "        output_bias: bool\n",
    "            If True, dense layers will use bias vectors in the self-attention layer.\n",
    "        d_input: int\n",
    "            Size of input layer in the feed-forward layer.\n",
    "        d_hidden: int\n",
    "            Size of hidden layer in the feed-forward layer. Will also be used as d_output for the MATEmbedding layer.\n",
    "        d_output: int\n",
    "            Size of output layer in the feed-forward layer.\n",
    "        activation: str\n",
    "            Activation function to be used in the feed-forward layer.\n",
    "            Can choose between 'relu' for ReLU, 'leakyrelu' for LeakyReLU, 'prelu' for PReLU,\n",
    "            'tanh' for TanH, 'selu' for SELU, 'elu' for ELU and 'linear' for linear activation.\n",
    "        n_layers: int\n",
    "            Number of layers in the feed-forward layer.\n",
    "        ff_dropout_p: float\n",
    "            Dropout probability in the feeed-forward layer.\n",
    "        encoder_hsize: int\n",
    "            Size of Dense layer for the encoder itself.\n",
    "        encoder_dropout_p: float\n",
    "            Dropout probability for connections in the encoder layer.\n",
    "        embed_input_hsize: int\n",
    "            Size of input layer for the MATEmbedding layer.\n",
    "        embed_dropout_p: float\n",
    "            Dropout probability for the MATEmbedding layer.\n",
    "        gen_aggregation_type: str\n",
    "            Type of aggregation to be used. Can be 'grover', 'mean' or 'contextual'.\n",
    "        gen_dropout_p: float\n",
    "            Dropout probability for the MATGenerator layer.\n",
    "        gen_n_layers: int\n",
    "            Number of layers in MATGenerator.\n",
    "        gen_attn_hidden: int\n",
    "            Size of hidden attention layer in the MATGenerator layer.\n",
    "        gen_attn_out: int\n",
    "            Size of output attention layer in the MATGenerator layer.\n",
    "        gen_d_output: int\n",
    "            Size of output layer in the MATGenerator layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MAT, self).__init__()\n",
    "\n",
    "        self.embedding = layers.MATEmbedding(d_input=embed_input_hsize,\n",
    "                                             d_output=d_hidden,\n",
    "                                             dropout_p=embed_dropout_p)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            layers.MATEncoderLayer(dist_kernel=dist_kernel,\n",
    "                                   lambda_attention=lambda_attention,\n",
    "                                   lambda_distance=lambda_distance,\n",
    "                                   h=h,\n",
    "                                   sa_hsize=sa_hsize,\n",
    "                                   sa_dropout_p=sa_dropout_p,\n",
    "                                   output_bias=output_bias,\n",
    "                                   d_input=d_input,\n",
    "                                   d_hidden=d_hidden,\n",
    "                                   d_output=d_output,\n",
    "                                   activation=activation,\n",
    "                                   n_layers=n_layers,\n",
    "                                   ff_dropout_p=ff_dropout_p,\n",
    "                                   encoder_hsize=encoder_hsize,\n",
    "                                   encoder_dropout_p=encoder_dropout_p)\n",
    "            for _ in range(n_encoders)\n",
    "        ])\n",
    "\n",
    "        self.generator = layers.MATGenerator(\n",
    "            hsize=d_input,\n",
    "            aggregation_type=gen_aggregation_type,\n",
    "            d_output=gen_d_output,\n",
    "            n_layers=gen_n_layers,\n",
    "            dropout_p=gen_dropout_p,\n",
    "            attn_hidden=gen_attn_hidden,\n",
    "            attn_out=gen_attn_out)\n",
    "\n",
    "    def forward(self, data: np.ndarray, **kwargs):\n",
    "        node_features = torch.tensor(data[0]).float()\n",
    "        adjacency_matrix = torch.tensor(data[1]).float()\n",
    "        distance_matrix = torch.tensor(data[2]).float()\n",
    "\n",
    "        mask = torch.sum(torch.abs(node_features), dim=-1) != 0\n",
    "        output = self.embedding(node_features)\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            output = layer(output, mask, adjacency_matrix, distance_matrix)\n",
    "        output = self.generator(output, mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MATModel(TorchModel):\n",
    "    \"\"\"Molecular Attention Transformer.\n",
    "\n",
    "    This class implements the Molecular Attention Transformer [1]_.\n",
    "    The MATFeaturizer (deepchem.feat.MATFeaturizer) is intended to work with this class.\n",
    "    The model takes a batch of MATEncodings (from MATFeaturizer) as input, and returns an array of size Nx1, where N is the number of molecules in the batch.\n",
    "    Each molecule is broken down into its Node Features matrix, adjacency matrix and distance matrix.\n",
    "    A mask tensor is calculated for the batch. All of this goes as input to the MATEmbedding, MATEncoder and MATGenerator layers, which are defined in deepchem.models.torch_models.layers.py\n",
    "\n",
    "    Currently, MATModel is intended to be a regression model for the freesolv dataset.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Lukasz Maziarka et al. \"Molecule Attention Transformer\" Graph Representation Learning workshop and Machine Learning and the Physical Sciences workshop at NeurIPS 2019. 2020. https://arxiv.org/abs/2002.08264\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import deepchem as dc\n",
    "    >>> import pandas as pd\n",
    "    >>> smiles = ['CC', 'CCC',  'CCCC', 'CCCCC', 'CCCCCCC']\n",
    "    >>> vals = [1.35, 6.72, 5.67, 1.23, 1.76]\n",
    "    >>> df = pd.DataFrame(list(zip(smiles, vals)), columns = ['smiles', 'y'])\n",
    "    >>> loader = dc.data.CSVLoader(tasks=['y'], feature_field='smiles', featurizer=dc.feat.MATFeaturizer())\n",
    "    >>> df.to_csv('test.csv')\n",
    "    >>> dataset = loader.create_dataset('test.csv')\n",
    "    >>> model = dc.models.torch_models.MATModel(batch_size = 2)\n",
    "    >>> out = model.fit(dataset, nb_epoch = 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dist_kernel: str = 'softmax',\n",
    "                 n_encoders=8,\n",
    "                 lambda_attention: float = 0.33,\n",
    "                 lambda_distance: float = 0.33,\n",
    "                 h: int = 16,\n",
    "                 sa_hsize: int = 1024,\n",
    "                 sa_dropout_p: float = 0.0,\n",
    "                 output_bias: bool = True,\n",
    "                 d_input: int = 1024,\n",
    "                 d_hidden: int = 1024,\n",
    "                 d_output: int = 1024,\n",
    "                 activation: str = 'leakyrelu',\n",
    "                 n_layers: int = 1,\n",
    "                 ff_dropout_p: float = 0.0,\n",
    "                 encoder_hsize: int = 1024,\n",
    "                 encoder_dropout_p: float = 0.0,\n",
    "                 embed_input_hsize: int = 36,\n",
    "                 embed_dropout_p: float = 0.0,\n",
    "                 gen_aggregation_type: str = 'mean',\n",
    "                 gen_dropout_p: float = 0.0,\n",
    "                 gen_n_layers: int = 1,\n",
    "                 gen_attn_hidden: int = 128,\n",
    "                 gen_attn_out: int = 4,\n",
    "                 gen_d_output: int = 1,\n",
    "                 **kwargs):\n",
    "        \"\"\"The wrapper class for the Molecular Attention Transformer.\n",
    "\n",
    "        Since we are using a custom data class as input (MATEncoding), we have overriden the default_generator function from DiskDataset and customized it to work with a batch of MATEncoding classes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dist_kernel: str\n",
    "            Kernel activation to be used. Can be either 'softmax' for softmax or 'exp' for exponential, for the self-attention layer.\n",
    "        n_encoders: int\n",
    "            Number of encoder layers in the encoder block.\n",
    "        lambda_attention: float\n",
    "            Constant to be multiplied with the attention matrix in the self-attention layer.\n",
    "        lambda_distance: float\n",
    "            Constant to be multiplied with the distance matrix in the self-attention layer.\n",
    "        h: int\n",
    "            Number of attention heads for the self-attention layer.\n",
    "        sa_hsize: int\n",
    "            Size of dense layer in the self-attention layer.\n",
    "        sa_dropout_p: float\n",
    "            Dropout probability for the self-attention layer.\n",
    "        output_bias: bool\n",
    "            If True, dense layers will use bias vectors in the self-attention layer.\n",
    "        d_input: int\n",
    "            Size of input layer in the feed-forward layer.\n",
    "        d_hidden: int\n",
    "            Size of hidden layer in the feed-forward layer. Will also be used as d_output for the MATEmbedding layer.\n",
    "        d_output: int\n",
    "            Size of output layer in the feed-forward layer.\n",
    "        activation: str\n",
    "            Activation function to be used in the feed-forward layer.\n",
    "            Can choose between 'relu' for ReLU, 'leakyrelu' for LeakyReLU, 'prelu' for PReLU,\n",
    "            'tanh' for TanH, 'selu' for SELU, 'elu' for ELU and 'linear' for linear activation.\n",
    "        n_layers: int\n",
    "            Number of layers in the feed-forward layer.\n",
    "        ff_dropout_p: float\n",
    "            Dropout probability in the feeed-forward layer.\n",
    "        encoder_hsize: int\n",
    "            Size of Dense layer for the encoder itself.\n",
    "        encoder_dropout_p: float\n",
    "            Dropout probability for connections in the encoder layer.\n",
    "        embed_input_hsize: int\n",
    "            Size of input layer for the MATEmbedding layer.\n",
    "        embed_dropout_p: float\n",
    "            Dropout probability for the MATEmbedding layer.\n",
    "        gen_aggregation_type: str\n",
    "            Type of aggregation to be used. Can be 'grover', 'mean' or 'contextual'.\n",
    "        gen_dropout_p: float\n",
    "            Dropout probability for the MATGenerator layer.\n",
    "        gen_n_layers: int\n",
    "            Number of layers in MATGenerator.\n",
    "        gen_attn_hidden: int\n",
    "            Size of hidden attention layer in the MATGenerator layer.\n",
    "        gen_attn_out: int\n",
    "            Size of output attention layer in the MATGenerator layer.\n",
    "        gen_d_output: int\n",
    "            Size of output layer in the MATGenerator layer.\n",
    "        \"\"\"\n",
    "        model = MAT(dist_kernel=dist_kernel,\n",
    "                    n_encoders=n_encoders,\n",
    "                    lambda_attention=lambda_attention,\n",
    "                    lambda_distance=lambda_distance,\n",
    "                    h=h,\n",
    "                    sa_hsize=sa_hsize,\n",
    "                    sa_dropout_p=sa_dropout_p,\n",
    "                    output_bias=output_bias,\n",
    "                    d_input=d_input,\n",
    "                    d_hidden=d_hidden,\n",
    "                    d_output=d_output,\n",
    "                    activation=activation,\n",
    "                    n_layers=n_layers,\n",
    "                    ff_dropout_p=ff_dropout_p,\n",
    "                    encoder_hsize=encoder_hsize,\n",
    "                    encoder_dropout_p=encoder_dropout_p,\n",
    "                    embed_input_hsize=embed_input_hsize,\n",
    "                    embed_dropout_p=embed_dropout_p,\n",
    "                    gen_aggregation_type=gen_aggregation_type,\n",
    "                    gen_dropout_p=gen_dropout_p,\n",
    "                    gen_n_layers=gen_n_layers,\n",
    "                    gen_attn_hidden=gen_attn_hidden,\n",
    "                    gen_attn_out=gen_attn_out,\n",
    "                    gen_d_output=gen_d_output)\n",
    "\n",
    "        loss = L2Loss()\n",
    "        output_types = ['prediction']\n",
    "        super(MATModel, self).__init__(model,\n",
    "                                       loss=loss,\n",
    "                                       output_types=output_types,\n",
    "                                       **kwargs)\n",
    "\n",
    "    def pad_array(self, array: np.ndarray, shape: Any) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pads an array to the desired shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        array: np.ndarray\n",
    "        Array to be padded.\n",
    "        shape: int or Tuple\n",
    "        Shape the array is padded to.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        array: np.ndarray\n",
    "        Array padded to input shape.\n",
    "        \"\"\"\n",
    "        result = np.zeros(shape=shape)\n",
    "        slices = tuple(slice(s) for s in array.shape)\n",
    "        result[slices] = array\n",
    "        return result\n",
    "\n",
    "    def pad_sequence(self, sequence: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pads a given sequence using the pad_array function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence: np.ndarray\n",
    "        Arrays in this sequence are padded to the largest shape in the sequence.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        array: np.ndarray\n",
    "        Sequence with padded arrays.\n",
    "        \"\"\"\n",
    "        shapes = np.stack([np.array(t.shape) for t in sequence])\n",
    "        max_shape = tuple(np.max(shapes, axis=0))\n",
    "        return np.stack([self.pad_array(t, shape=max_shape) for t in sequence])\n",
    "\n",
    "    def default_generator(self,\n",
    "                          dataset,\n",
    "                          epochs=1,\n",
    "                          mode='fit',\n",
    "                          deterministic=True,\n",
    "                          pad_batches=True,\n",
    "                          **kwargs):\n",
    "        for epoch in range(epochs):\n",
    "            for (X_b, y_b, w_b,\n",
    "                 ids_b) in dataset.iterbatches(batch_size=self.batch_size,\n",
    "                                               deterministic=deterministic,\n",
    "                                               pad_batches=pad_batches):\n",
    "\n",
    "                node_features = self.pad_sequence(\n",
    "                    [torch.tensor(data.node_features).float() for data in X_b])\n",
    "                adjacency_matrix = self.pad_sequence([\n",
    "                    torch.tensor(data.adjacency_matrix).float() for data in X_b\n",
    "                ])\n",
    "                distance_matrix = self.pad_sequence([\n",
    "                    torch.tensor(data.distance_matrix).float() for data in X_b\n",
    "                ])\n",
    "\n",
    "                inputs = [node_features, adjacency_matrix, distance_matrix]\n",
    "                yield (inputs, [y_b], [w_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models.torch_models import MATModel\n",
    "from deepchem.feat import MATFeaturizer\n",
    "import matplotlib.pyplot as plt\n",
    "#import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deepchem.feat.molecule_featurizers.mat_featurizer.MATEncoding'>\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]]\n",
      "[[1.e+06 1.e+06 1.e+06 1.e+06 1.e+06 1.e+06]\n",
      " [1.e+06 0.e+00 4.e+00 3.e+00 2.e+00 1.e+00]\n",
      " [1.e+06 4.e+00 0.e+00 1.e+00 2.e+00 3.e+00]\n",
      " [1.e+06 3.e+00 1.e+00 0.e+00 1.e+00 2.e+00]\n",
      " [1.e+06 2.e+00 2.e+00 1.e+00 0.e+00 1.e+00]\n",
      " [1.e+06 1.e+00 3.e+00 2.e+00 1.e+00 0.e+00]]\n"
     ]
    }
   ],
   "source": [
    "featurizer = dc.feat.MATFeaturizer()\n",
    "# Let us now take an example array of smile strings and featurize it.\n",
    "smile_string = [\"COCCOc1cc(=O)n2c(c1C(=O)N1CCCC1)CCOCC2\"]\n",
    "output = featurizer.featurize(smile_string)\n",
    "print(type(output[0]))\n",
    "nf = output[0].node_features\n",
    "print(nf)\n",
    "am = output[0].adjacency_matrix\n",
    "print(am)\n",
    "dm = output[0].distance_matrix\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = MATModel(device = device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
