{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW,Adam\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def count_parameters_per_layer(model):\n",
    "    param_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        param_counts[name] = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    return param_counts\n",
    "\n",
    "# RMSE 손실 함수 정의\n",
    "def rmse_loss(y_pred, y_true):\n",
    "    mse = torch.nn.MSELoss()(y_pred, y_true)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서로 변환, SMILES토큰화\n",
    "class SMILESDataset(Dataset):\n",
    "    #변수선언\n",
    "    def __init__(self,smile_list,mlm_labels,hlm_labels,max_length,tokenizer):\n",
    "        self.smile_list = smile_list.tolist()\n",
    "        self.mlm_labels = mlm_labels.tolist()\n",
    "        self.hlm_labels = hlm_labels.tolist()\n",
    "        self.tokenizer = tokenizer.encode_plus\n",
    "        self.max_length = max_length\n",
    "    #smiles의 길이 출력 (편의)\n",
    "    def __len__(self):\n",
    "        return len(self.smile_list)\n",
    "    #attention변환 및 tensor변환\n",
    "    def __getitem__(self, index):\n",
    "        mlm_label = torch.tensor(self.mlm_labels[index], dtype=torch.float)\n",
    "        hlm_label = torch.tensor(self.hlm_labels[index], dtype=torch.float)\n",
    "        inputs = self.tokenizer(\n",
    "                    self.smile_list[index],\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,  # 최대 길이 설정\n",
    "                    padding='max_length',  # 패딩 옵션 추가 SMILES마다 길이가 다른걸 맞춰줌\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True\n",
    "                    )\n",
    "        del inputs['token_type_ids']  # token_type_ids를 제거\n",
    "        input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long)\n",
    "    \n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, mlm_label, hlm_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBasedRegressor(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(AttentionBasedRegressor, self).__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(pretrained_model_name, output_attentions=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(self.encoder.config.hidden_size, self.encoder.config.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 최종 예측을 위한 Linear 레이어\n",
    "        self.regressor = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        attention_scores = outputs.attentions[-1]\n",
    "        attention_weights = attention_scores.mean(dim=1)\n",
    "        attention_weights_avg = attention_weights.mean(dim=-1)\n",
    "        weighted_avg = torch.sum(sequence_output * attention_weights_avg.unsqueeze(-1), dim=1)\n",
    "\n",
    "        x = self.fc1(weighted_avg)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        mlm_prediction = self.regressor(x)\n",
    "        hlm_prediction = self.regressor(x)\n",
    "        return mlm_prediction, hlm_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Size Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding_layers: 5\n",
      "decoding_parameters: 591361\n"
     ]
    }
   ],
   "source": [
    "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "model = AttentionBasedRegressor(model_version).to(device)\n",
    "layer_params = count_parameters_per_layer(model)\n",
    "\n",
    "print(\"decoding_layers:\",sum(1 for _ in model.modules()) - sum(1 for _ in model.encoder.modules()))\n",
    "print(\"decoding_parameters:\",sum(p.numel() for p in model.parameters()) - sum(p.numel() for p in model.encoder.parameters()))\n",
    "# for layer_name, param_count in layer_params.items():\n",
    "#     print(f\"{layer_name}: {param_count} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./origin_data/train.csv')\n",
    "test_df = pd.read_csv('./origin_data/test.csv')\n",
    "train_max = train_df['SMILES'].astype(str).apply(len).max()\n",
    "test_max = test_df['SMILES'].astype(str).apply(len).max()\n",
    "print(train_max)\n",
    "print(test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Workspace\\New_drug_development\\wandb\\run-20230920_181034-8ymdlj6u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/drugformer/runs/8ymdlj6u' target=\"_blank\">trim-night-14</a></strong> to <a href='https://wandb.ai/waooang/drugformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/drugformer' target=\"_blank\">https://wandb.ai/waooang/drugformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/drugformer/runs/8ymdlj6u' target=\"_blank\">https://wandb.ai/waooang/drugformer/runs/8ymdlj6u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:14<07:02, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Valid MLM Loss: 54.88081359863281, Valid HLM Loss: 72.0864486694336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 2/30 [00:29<06:45, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Valid MLM Loss: 46.8592643737793, Valid HLM Loss: 62.30195617675781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 3/30 [00:43<06:27, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Valid MLM Loss: 38.367889404296875, Valid HLM Loss: 49.345314025878906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 4/30 [00:57<06:09, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Valid MLM Loss: 36.09564208984375, Valid HLM Loss: 43.07697296142578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 5/30 [01:11<05:54, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Valid MLM Loss: 35.92513656616211, Valid HLM Loss: 42.03936004638672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 6/30 [01:25<05:41, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Valid MLM Loss: 36.08451843261719, Valid HLM Loss: 43.11637878417969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 7/30 [01:39<05:24, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Valid MLM Loss: 35.822052001953125, Valid HLM Loss: 42.92634963989258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 8/30 [01:53<05:09, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Valid MLM Loss: 32.71649169921875, Valid HLM Loss: 43.061283111572266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 9/30 [02:07<04:54, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Valid MLM Loss: 31.195390701293945, Valid HLM Loss: 37.285362243652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 10/30 [02:21<04:40, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Valid MLM Loss: 31.056629180908203, Valid HLM Loss: 40.094947814941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:35<04:25, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Valid MLM Loss: 32.01328659057617, Valid HLM Loss: 36.203372955322266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 12/30 [02:49<04:11, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Valid MLM Loss: 32.34675979614258, Valid HLM Loss: 37.10889434814453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 13/30 [03:03<03:59, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Valid MLM Loss: 32.1978874206543, Valid HLM Loss: 35.8953971862793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 14/30 [03:18<03:47, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Valid MLM Loss: 33.25883102416992, Valid HLM Loss: 33.86742401123047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 15/30 [03:32<03:32, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Valid MLM Loss: 33.814002990722656, Valid HLM Loss: 35.366905212402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 16/30 [03:46<03:17, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Valid MLM Loss: 34.78998565673828, Valid HLM Loss: 36.78506088256836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 17/30 [04:00<03:05, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Valid MLM Loss: 34.76375961303711, Valid HLM Loss: 32.880706787109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 18/30 [04:14<02:50, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Valid MLM Loss: 35.174530029296875, Valid HLM Loss: 31.21741485595703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 19/30 [04:28<02:35, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Valid MLM Loss: 34.95778274536133, Valid HLM Loss: 33.11790084838867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 20/30 [04:42<02:21, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Valid MLM Loss: 35.343441009521484, Valid HLM Loss: 32.01578903198242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [04:56<02:06, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Valid MLM Loss: 35.56621170043945, Valid HLM Loss: 33.6020393371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 22/30 [05:10<01:52, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Valid MLM Loss: 35.75987243652344, Valid HLM Loss: 34.228302001953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 23/30 [05:25<01:38, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Valid MLM Loss: 35.79689025878906, Valid HLM Loss: 32.507293701171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 24/30 [05:39<01:24, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Valid MLM Loss: 35.87950134277344, Valid HLM Loss: 32.21288299560547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 25/30 [05:53<01:10, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Valid MLM Loss: 36.125179290771484, Valid HLM Loss: 31.331857681274414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 26/30 [06:07<00:56, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Valid MLM Loss: 36.308502197265625, Valid HLM Loss: 31.397075653076172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 27/30 [06:21<00:42, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Valid MLM Loss: 36.75664138793945, Valid HLM Loss: 31.78180694580078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 28/30 [06:35<00:28, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Valid MLM Loss: 36.848209381103516, Valid HLM Loss: 32.01435470581055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 29/30 [06:49<00:14, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Valid MLM Loss: 36.809303283691406, Valid HLM Loss: 31.71328353881836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [07:03<00:00, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Valid MLM Loss: 36.83705139160156, Valid HLM Loss: 31.648778915405273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train HLM Loss</td><td>█▄▅▄▃▅▄▄▄▃▃▂▂▄▃▂▂▄▃▄▃▁▁▃▃▁▂▂▃▃</td></tr><tr><td>Train MLM Loss</td><td>█▅▄▄▄▄▃▅▄▃▄▃▃▃▂▂▂▄▃▃▃▂▃▂▁▂▃▂▂▂</td></tr><tr><td>Train Total Loss</td><td>█▄▄▄▃▄▄▅▃▃▃▂▃▄▂▂▂▄▃▃▃▁▂▂▂▁▃▂▂▂</td></tr><tr><td>Valid HLM Loss</td><td>█▆▄▃▃▃▃▃▂▃▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid MLM Loss</td><td>█▆▃▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃</td></tr><tr><td>Valid Total Loss</td><td>█▆▃▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train HLM Loss</td><td>27.98411</td></tr><tr><td>Train MLM Loss</td><td>19.56586</td></tr><tr><td>Train Total Loss</td><td>23.77498</td></tr><tr><td>Valid HLM Loss</td><td>31.64878</td></tr><tr><td>Valid MLM Loss</td><td>36.83705</td></tr><tr><td>Valid Total Loss</td><td>34.24292</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-night-14</strong> at: <a href='https://wandb.ai/waooang/drugformer/runs/8ymdlj6u' target=\"_blank\">https://wandb.ai/waooang/drugformer/runs/8ymdlj6u</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230920_181034-8ymdlj6u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "MAX_LEN = 178\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "lr = 1e-5\n",
    "\n",
    "wandb.init(    \n",
    "           project='drugformer',\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Transformer\",\n",
    "    \"dataset\": \"Custom\",\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size\n",
    "    }\n",
    ")\n",
    "train_df = pd.read_csv('./origin_data/train.csv')\n",
    "# 데이터를 학습 및 검증 세트로 분할\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_version)\n",
    "# 학습 및 검증 데이터로더 생성\n",
    "train_dataset = SMILESDataset(train_df['SMILES'], train_df['MLM'], train_df['HLM'], MAX_LEN, tokenizer)\n",
    "valid_dataset = SMILESDataset(valid_df['SMILES'], valid_df['MLM'], valid_df['HLM'], MAX_LEN, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = AttentionBasedRegressor(model_version).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = rmse_loss\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "for epoch in tqdm(range(epochs),desc=\"Training\"):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs, mlm_label, hlm_label = batch\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        mlm_label, hlm_label = mlm_label.to(device), hlm_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mlm_pred, hlm_pred = model(**inputs)\n",
    "        #rmse계산\n",
    "        train_loss_mlm = criterion(mlm_pred.squeeze(), mlm_label)\n",
    "        train_loss_hlm = criterion(hlm_pred.squeeze(), hlm_label)\n",
    "        train_total_loss = train_loss_mlm*0.5 + train_loss_hlm*0.5\n",
    "        train_total_loss.backward()#역전파\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 학습률 스케줄러 업데이트\n",
    "    # 검증 부분\n",
    "    model.eval()\n",
    "    valid_loss_mlm = 0\n",
    "    valid_loss_hlm = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            inputs, mlm_label, hlm_label = batch\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            mlm_label, hlm_label = mlm_label.to(device), hlm_label.to(device)\n",
    "            \n",
    "            mlm_pred, hlm_pred = model(**inputs)\n",
    "            loss_mlm = criterion(mlm_pred.squeeze(), mlm_label)\n",
    "            loss_hlm = criterion(hlm_pred.squeeze(), hlm_label)\n",
    "            valid_total_loss = loss_hlm*0.5 + loss_mlm*0.5\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Valid MLM Loss: {loss_mlm}, Valid HLM Loss: {loss_hlm}\")\n",
    "    wandb.log({'Train Total Loss': train_total_loss, 'Valid Total Loss': valid_total_loss,'Train MLM Loss': train_loss_mlm, 'Train HLM Loss': train_loss_hlm, 'Valid MLM Loss': loss_mlm, 'Valid HLM Loss': loss_hlm})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제출물 이름\n",
    "NAME = \"transformer_attention_feat_linear_relu_batch64\"\n",
    "# 1. 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 2. 예측을 위한 입력 데이터 준비\n",
    "test_df = pd.read_csv('./origin_data/test.csv')\n",
    "test_smiles = test_df['SMILES'].tolist()\n",
    "encoded_inputs = tokenizer(test_smiles, return_tensors=\"pt\", max_length=MAX_LEN,padding=True, truncation=True)\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# 3. 예측 수행\n",
    "with torch.no_grad():\n",
    "    mlm_preds, hlm_preds = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# 4. 제출\n",
    "mlm_predictions = mlm_preds.cpu().numpy()\n",
    "hlm_predictions = hlm_preds.cpu().numpy()\n",
    "\n",
    "submission = pd.read_csv('./origin_data/sample_submission.csv')\n",
    "submission['MLM'] = mlm_predictions\n",
    "submission['HLM'] = hlm_predictions\n",
    "submission.to_csv(f'./submission/{NAME}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./transformer_model/{NAME}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파라미터 로드\n",
    "# model.load_state_dict(torch.load('model_parameters.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
