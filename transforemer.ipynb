{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW,Adam\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def count_parameters_per_layer(model):\n",
    "    param_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        param_counts[name] = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    return param_counts\n",
    "\n",
    "# RMSE 손실 함수 정의\n",
    "def rmse_loss(y_pred, y_true):\n",
    "    mse = torch.nn.MSELoss()(y_pred, y_true)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서로 변환, SMILES토큰화\n",
    "class SMILESDataset(Dataset):\n",
    "    #변수선언\n",
    "    def __init__(self,smile_list,mlm_labels,hlm_labels,max_length,tokenizer):\n",
    "        self.smile_list = smile_list.tolist()\n",
    "        self.mlm_labels = mlm_labels.tolist()\n",
    "        self.hlm_labels = hlm_labels.tolist()\n",
    "        self.tokenizer = tokenizer.encode_plus\n",
    "        self.max_length = max_length\n",
    "    #smiles의 길이 출력 (편의)\n",
    "    def __len__(self):\n",
    "        return len(self.smile_list)\n",
    "    #attention변환 및 tensor변환\n",
    "    def __getitem__(self, index):\n",
    "        mlm_label = torch.tensor(self.mlm_labels[index], dtype=torch.float)\n",
    "        hlm_label = torch.tensor(self.hlm_labels[index], dtype=torch.float)\n",
    "        inputs = self.tokenizer(\n",
    "                    self.smile_list[index],\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,  # 최대 길이 설정\n",
    "                    padding='max_length',  # 패딩 옵션 추가 SMILES마다 길이가 다른걸 맞춰줌\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True\n",
    "                    )\n",
    "        del inputs['token_type_ids']  # token_type_ids를 제거\n",
    "        input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long)\n",
    "    \n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, mlm_label, hlm_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBasedRegressor(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(AttentionBasedRegressor, self).__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(pretrained_model_name, output_attentions=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(self.encoder.config.hidden_size, self.encoder.config.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 최종 예측을 위한 Linear 레이어\n",
    "        self.regressor = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        attention_scores = outputs.attentions[-1]\n",
    "        attention_weights = attention_scores.mean(dim=1)\n",
    "        attention_weights_avg = attention_weights.mean(dim=-1)\n",
    "        weighted_avg = torch.sum(sequence_output * attention_weights_avg.unsqueeze(-1), dim=1)\n",
    "\n",
    "\n",
    "        x = self.fc1(weighted_avg)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mlm_prediction = self.regressor(x)\n",
    "        hlm_prediction = self.regressor(x)\n",
    "        return mlm_prediction, hlm_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Size Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding_layers: 5\n",
      "decoding_parameters: 591361\n"
     ]
    }
   ],
   "source": [
    "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "model = AttentionBasedRegressor(model_version).to(device)\n",
    "layer_params = count_parameters_per_layer(model)\n",
    "\n",
    "print(\"decoding_layers:\",sum(1 for _ in model.modules()) - sum(1 for _ in model.encoder.modules()))\n",
    "print(\"decoding_parameters:\",sum(p.numel() for p in model.parameters()) - sum(p.numel() for p in model.encoder.parameters()))\n",
    "# for layer_name, param_count in layer_params.items():\n",
    "#     print(f\"{layer_name}: {param_count} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./origin_data/train.csv')\n",
    "test_df = pd.read_csv('./origin_data/test.csv')\n",
    "train_max = train_df['SMILES'].astype(str).apply(len).max()\n",
    "test_max = test_df['SMILES'].astype(str).apply(len).max()\n",
    "print(train_max)\n",
    "print(test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w8dlddlm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-darkness-1</strong> at: <a href='https://wandb.ai/waooang/my-awesome-project/runs/w8dlddlm' target=\"_blank\">https://wandb.ai/waooang/my-awesome-project/runs/w8dlddlm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230918_024650-w8dlddlm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w8dlddlm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Workspace\\New_drug_development\\wandb\\run-20230918_025204-pi8a61pf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/drugformer/runs/pi8a61pf' target=\"_blank\">giddy-wave-4</a></strong> to <a href='https://wandb.ai/waooang/drugformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/drugformer' target=\"_blank\">https://wandb.ai/waooang/drugformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/drugformer/runs/pi8a61pf' target=\"_blank\">https://wandb.ai/waooang/drugformer/runs/pi8a61pf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pi8a61pf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-wave-4</strong> at: <a href='https://wandb.ai/waooang/drugformer/runs/pi8a61pf' target=\"_blank\">https://wandb.ai/waooang/drugformer/runs/pi8a61pf</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230918_025204-pi8a61pf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pi8a61pf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Workspace\\New_drug_development\\wandb\\run-20230918_025211-6uoixn34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/my-awesome-project/runs/6uoixn34' target=\"_blank\">super-wave-2</a></strong> to <a href='https://wandb.ai/waooang/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/my-awesome-project' target=\"_blank\">https://wandb.ai/waooang/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/my-awesome-project/runs/6uoixn34' target=\"_blank\">https://wandb.ai/waooang/my-awesome-project/runs/6uoixn34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 1/10 [00:14<02:10, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Total Loss: 0.43960899114608765 MLM_Loss:0.39234660755504264 HLM Loss:0.4868714072487571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2/10 [00:28<01:54, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Total Loss: 0.3818061351776123 MLM_Loss:0.2852050261064009 HLM Loss:0.47840728759765627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3/10 [00:42<01:39, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Total Loss: 0.36632204055786133 MLM_Loss:0.2918754924427379 HLM Loss:0.4407685713334517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4/10 [00:57<01:25, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Total Loss: 0.3844561278820038 MLM_Loss:0.37461190657182175 HLM Loss:0.39430039145729756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5/10 [01:11<01:11, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Total Loss: 0.33576780557632446 MLM_Loss:0.3372519406405362 HLM Loss:0.33428369001908737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 6/10 [01:26<00:57, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Total Loss: 0.40697982907295227 MLM_Loss:0.40199168812144886 HLM Loss:0.4119680578058416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7/10 [01:40<00:42, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Total Loss: 0.36764761805534363 MLM_Loss:0.383680551702326 HLM Loss:0.351614691994407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8/10 [01:54<00:28, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Total Loss: 0.2719874083995819 MLM_Loss:0.25629914023659445 HLM Loss:0.28767573616721415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9/10 [02:08<00:14, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Total Loss: 0.3407610356807709 MLM_Loss:0.3253228967840021 HLM Loss:0.35619919516823506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [02:23<00:00, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Total Loss: 0.3600318133831024 MLM_Loss:0.34064781882546163 HLM Loss:0.37941585887562146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>HLM Loss</td><td>▇▇█▆▆▄▅▇▆▅▄▇▆▄▄▅▃▄▃▄▃▅▄▃▂▃▄▃▃▄▃▂▁▃▃▃▃▂▃▃</td></tr><tr><td>MLM_Loss</td><td>▆▇█▆▆▆▅▆▆▄▄▇▅▃▅▅▄▄▂▅▂▄▃▄▂▃▄▄▃▃▃▂▁▄▃▃▄▃▂▂</td></tr><tr><td>Total Loss</td><td>▇▇█▆▆▅▅▇▆▅▄▇▆▄▄▅▄▄▃▅▂▄▄▃▂▃▄▃▃▃▃▂▁▄▃▃▄▃▃▃</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>HLM Loss</td><td>0.37942</td></tr><tr><td>MLM_Loss</td><td>0.34065</td></tr><tr><td>Total Loss</td><td>0.36003</td></tr><tr><td>epoch</td><td>9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-wave-2</strong> at: <a href='https://wandb.ai/waooang/my-awesome-project/runs/6uoixn34' target=\"_blank\">https://wandb.ai/waooang/my-awesome-project/runs/6uoixn34</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230918_025211-6uoixn34\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "MAX_LEN = 178\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr = 1e-5\n",
    "wandb.init(project='drugformer')\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Transformer\",\n",
    "    \"dataset\": \"Custom\",\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size\n",
    "    }\n",
    ")\n",
    "train_df = pd.read_csv('./origin_data/train.csv')\n",
    "# 데이터셋 분할\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "smiles_list_train = train_df['SMILES']\n",
    "mlm_labels = train_df['MLM']\n",
    "hlm_labels = train_df['HLM']\n",
    "\n",
    "\n",
    "\n",
    "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_version)\n",
    "dataset = SMILESDataset(smiles_list_train, mlm_labels, hlm_labels, MAX_LEN, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = AttentionBasedRegressor(model_version).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = rmse_loss\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=len(dataloader) * epochs)\n",
    "\n",
    "for epoch in tqdm(range(epochs),desc=\"Training\"):\n",
    "    for batch in dataloader:\n",
    "        inputs, mlm_label, hlm_label = batch\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        mlm_label, hlm_label = mlm_label.to(device), hlm_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mlm_pred, hlm_pred = model(**inputs)\n",
    "        #rmse계산\n",
    "        loss_mlm = criterion(mlm_pred.squeeze(), mlm_label)\n",
    "        loss_hlm = criterion(hlm_pred.squeeze(), hlm_label)\n",
    "        total_loss = loss_mlm*0.5 + loss_hlm*0.5\n",
    "        total_loss.backward()#역전파\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 학습률 스케줄러 업데이트\n",
    "        wandb.log({'Total Loss': total_loss/len(dataloader) ,'MLM_Loss':loss_mlm.item()/len(dataloader) ,'HLM Loss':loss_hlm.item()/len(dataloader),'epoch':epoch})\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {total_loss/len(dataloader)} MLM_Loss:{loss_mlm.item()/len(dataloader)} HLM Loss:{loss_hlm.item()/len(dataloader)}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제출물 이름\n",
    "NAME = \"transformer_attention_feat_linear_relu_batch64\"\n",
    "# 1. 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 2. 예측을 위한 입력 데이터 준비\n",
    "test_df = pd.read_csv('./origin_data/test.csv')\n",
    "test_smiles = test_df['SMILES'].tolist()\n",
    "encoded_inputs = tokenizer(test_smiles, return_tensors=\"pt\", max_length=MAX_LEN,padding=True, truncation=True)\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# 3. 예측 수행\n",
    "with torch.no_grad():\n",
    "    mlm_preds, hlm_preds = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# 4. 제출\n",
    "mlm_predictions = mlm_preds.cpu().numpy()\n",
    "hlm_predictions = hlm_preds.cpu().numpy()\n",
    "\n",
    "submission = pd.read_csv('./origin_data/sample_submission.csv')\n",
    "submission['MLM'] = mlm_predictions\n",
    "submission['HLM'] = hlm_predictions\n",
    "submission.to_csv(f'./submission/{NAME}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'./transformer_model/{NAME}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('full_model.pth')\n",
    "# model.eval()  # 모델을 평가 모드로 설정 (필요한 경우)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
