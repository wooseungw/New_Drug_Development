{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW,Adam\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def count_parameters_per_layer(model):\n",
    "    param_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        param_counts[name] = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    return param_counts\n",
    "\n",
    "# RMSE 손실 함수 정의\n",
    "def rmse_loss(y_pred, y_true):\n",
    "    mse = torch.nn.MSELoss()(y_pred, y_true)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, df, max_length, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer.encode_plus\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 데이터를 텐서로 변환\n",
    "        self.features_tensors = []\n",
    "        self.mlm_labels = []\n",
    "        self.hlm_labels = []\n",
    "        self.input_ids_list = []\n",
    "        self.attention_masks_list = []\n",
    "        \n",
    "        features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "        \n",
    "        features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea']\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            # 데이터 타입을 float로 변환\n",
    "            row_values = row[features].astype(float).values\n",
    "            features_tensor = torch.tensor(row_values, dtype=torch.float)\n",
    "            mlm_label = torch.tensor(row['MLM'], dtype=torch.float)\n",
    "            hlm_label = torch.tensor(row['HLM'], dtype=torch.float)\n",
    "            inputs = self.tokenizer(\n",
    "                row['SMILES'],\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long)\n",
    "            attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long)\n",
    "            \n",
    "            self.features_tensors.append(features_tensor)\n",
    "            self.mlm_labels.append(mlm_label)\n",
    "            self.hlm_labels.append(hlm_label)\n",
    "            self.input_ids_list.append(input_ids)\n",
    "            self.attention_masks_list.append(attention_mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\"input_ids\": self.input_ids_list[index], \"attention_mask\": self.attention_masks_list[index], 'features_tensor': self.features_tensors[index]}, self.mlm_labels[index], self.hlm_labels[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class AttentionBasedRegressor(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, feature_dim):\n",
    "        super(AttentionBasedRegressor, self).__init__()\n",
    "        \n",
    "        self.encoder = RobertaModel.from_pretrained(pretrained_model_name, output_attentions=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # Attention score의 가중 평균을 사용하는 리니어\n",
    "        self.linear_attention = nn.Linear(self.encoder.config.hidden_size, self.encoder.config.hidden_size)\n",
    "        # 추가적인 특징을 학습하는 리니어\n",
    "        self.linear_features = nn.Linear(feature_dim,feature_dim)\n",
    "        self.linear_feat_to_regressor = nn.Linear(feature_dim, self.encoder.config.hidden_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        # 두 출력을 결합하여 최종 출력을 생성하는 리니어\n",
    "        self.regressor = nn.Linear(self.encoder.config.hidden_size * 2, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, features=None):\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        attention_scores = outputs.attentions[-1]\n",
    "        attention_weights = attention_scores.mean(dim=1)\n",
    "        attention_weights_avg = attention_weights.mean(dim=-1)\n",
    "        weighted_avg = torch.sum(sequence_output * attention_weights_avg.unsqueeze(-1), dim=1)\n",
    "\n",
    "        out_attention = self.relu(self.linear_attention(weighted_avg))\n",
    "        \n",
    "        out_features = self.relu(self.linear_features(features))\n",
    "        out_features = self.relu(self.linear_features(out_features))\n",
    "        out_features = self.relu(self.linear_feat_to_regressor(out_features))\n",
    "        \n",
    "        combined = torch.cat((out_attention, out_features), dim=1)\n",
    "        combined = self.dropout(out_features)\n",
    "        \n",
    "        prediction = self.regressor(out_features)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Size Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding_layers: 6\n",
      "decoding_parameters: 598273\n"
     ]
    }
   ],
   "source": [
    "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "model = AttentionBasedRegressor(model_version,7).to(device)\n",
    "layer_params = count_parameters_per_layer(model)\n",
    "\n",
    "print(\"decoding_layers:\",sum(1 for _ in model.modules()) - sum(1 for _ in model.encoder.modules()))\n",
    "print(\"decoding_parameters:\",sum(p.numel() for p in model.parameters()) - sum(p.numel() for p in model.encoder.parameters()))\n",
    "# for layer_name, param_count in layer_params.items():\n",
    "#     print(f\"{layer_name}: {param_count} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv('./origin_data/train.csv')\n",
    "# test_df = pd.read_csv('./origin_data/test.csv')\n",
    "# train_max = train_df['SMILES'].astype(str).apply(len).max()\n",
    "# test_max = test_df['SMILES'].astype(str).apply(len).max()\n",
    "# print(train_max)\n",
    "# print(test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:11<00:23, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train MLM Loss: nan, Valid MLM Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:23<00:11, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train MLM Loss: nan, Valid MLM Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:34<00:00, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train MLM Loss: nan, Valid MLM Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "MAX_LEN = 180\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "\n",
    "# wandb.init(    \n",
    "#            project='drugformer',\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": lr,\n",
    "#     \"architecture\": \"Transformer-Linear\",\n",
    "#     \"dataset\": \"Custom\",\n",
    "#     \"epochs\": epochs,\n",
    "#     \"batch_size\": batch_size\n",
    "#     }\n",
    "# )\n",
    "train_df = pd.read_csv('./origin_data/train.csv').drop(columns=['id'])\n",
    "# 데이터를 학습 및 검증 세트로 분할\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "model_version = 'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_version)\n",
    "# 학습 및 검증 데이터로더 생성\n",
    "train_dataset = SMILESDataset(train_df, MAX_LEN, tokenizer)\n",
    "valid_dataset = SMILESDataset(valid_df, MAX_LEN, tokenizer)\n",
    "feat_dim = 7\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "mlm_model = AttentionBasedRegressor(model_version,feat_dim).to(device)\n",
    "hlm_model = AttentionBasedRegressor(model_version,feat_dim).to(device)\n",
    "mlm_optimizer = Adam(mlm_model.parameters(), lr=lr)\n",
    "hlm_optimizer = Adam(hlm_model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "criterion = rmse_loss\n",
    "scheduler = get_linear_schedule_with_warmup(mlm_optimizer, num_warmup_steps=5, num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "for epoch in tqdm(range(epochs),desc=\"Training\"):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs, mlm_label, hlm_label = batch\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        features_tensor = inputs[\"features_tensor\"].to(device)\n",
    "        mlm_label, hlm_label = mlm_label.to(device), hlm_label.to(device)\n",
    "        mlm_optimizer.zero_grad()        \n",
    "\n",
    "        mlm_pred = model(input_ids=input_ids, attention_mask=attention_mask, features=features_tensor)[0]\n",
    "\n",
    "        #rmse계산\n",
    "        train_loss_mlm = criterion(mlm_pred.squeeze(), mlm_label)\n",
    "        #train_loss_hlm = criterion(hlm_pred.squeeze(), hlm_label)\n",
    "        #train_total_loss = train_loss_mlm*0.5 + train_loss_hlm*0.5\n",
    "        train_loss_mlm.backward()#역전파\n",
    "        mlm_optimizer.step()\n",
    "        scheduler.step()  # 학습률 스케줄러 업데이트\n",
    "    \n",
    "    # 검증 부분\n",
    "    model.eval()\n",
    "    valid_loss_mlm = 0\n",
    "    valid_loss_hlm = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            inputs, mlm_label, hlm_label = batch\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "            features_tensor = inputs[\"features_tensor\"].to(device)\n",
    "            mlm_label, hlm_label = mlm_label.to(device), hlm_label.to(device)    \n",
    "                    \n",
    "            mlm_pred = model(input_ids=input_ids, attention_mask=attention_mask, features=features_tensor)[0]\n",
    "            loss_mlm = criterion(mlm_pred.squeeze(), mlm_label)\n",
    "            #loss_hlm = criterion(hlm_pred.squeeze(), hlm_label)\n",
    "            #valid_total_loss = loss_hlm*0.5 + loss_mlm*0.5\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train MLM Loss: {train_loss_mlm}, Valid MLM Loss: {loss_mlm}\")\n",
    "    #wandb.log({'Train Total Loss': train_total_loss, 'Valid Total Loss': valid_total_loss,'Train MLM Loss': train_loss_mlm, 'Train HLM Loss': train_loss_hlm, 'Valid MLM Loss': loss_mlm, 'Valid HLM Loss': loss_hlm})\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./transformer_model/{NAME}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파라미터 로드\n",
    "# model.load_state_dict(torch.load('model_parameters.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
